{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AlbertTokenizer, TFAlbertForSequenceClassification\n",
        "import tensorflow as tf\n",
        "\n",
        "# Configurations\n",
        "FILE_PATH = 'datasets/datasets_beauty_product.csv'\n",
        "EPOCH = 20\n",
        "BATCH = 32\n",
        "LEARNING_RATE = 1e-5\n",
        "MAX_LENGTH = 1000\n",
        "BASE_PRETRAINED_MODEL = 'albert-base-v2'\n",
        "NUM_LABELS = 3  # Adjust this if using more labels\n",
        "MODEL_PATH = \"models/my-product-data-202412221621.h5\"\n",
        "\n",
        "# Read and clean dataset\n",
        "df = pd.read_csv(FILE_PATH)\n",
        "valid_sentiments = ['Positive', 'Neutral', 'Negative']\n",
        "df = df[df['sentiment'].isin(valid_sentiments)]\n",
        "\n",
        "if df.empty:\n",
        "    raise ValueError(\"Dataset is empty after filtering. Check sentiment labels.\")\n",
        "\n",
        "# Label mapping\n",
        "label_mapping = {'Positive': 0, 'Neutral': 1, 'Negative': 2}\n",
        "\n",
        "# Prepare data\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df['reviews'].values,\n",
        "    df['sentiment'].values,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "train_labels_numeric = [label_mapping[label] for label in train_labels]\n",
        "test_labels_numeric = [label_mapping[label] for label in test_labels]\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = AlbertTokenizer.from_pretrained(BASE_PRETRAINED_MODEL)\n",
        "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
        "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
        "\n",
        "# Extract NumPy arrays\n",
        "train_input_ids = train_encodings['input_ids'].numpy()\n",
        "train_attention_mask = train_encodings['attention_mask'].numpy()\n",
        "test_input_ids = test_encodings['input_ids'].numpy()\n",
        "test_attention_mask = test_encodings['attention_mask'].numpy()\n",
        "\n",
        "# Create tf.data.Dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(((train_input_ids, train_attention_mask), train_labels_numeric)).batch(BATCH)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(((test_input_ids, test_attention_mask), test_labels_numeric)).batch(BATCH)\n",
        "\n",
        "# Load and compile model\n",
        "model = TFAlbertForSequenceClassification.from_pretrained(BASE_PRETRAINED_MODEL, num_labels=NUM_LABELS)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(train_dataset, epochs=EPOCH)\n",
        "\n",
        "# Evaluate model\n",
        "eval_results = model.evaluate(test_dataset)\n",
        "print(\"Test loss:\", eval_results[0])\n",
        "print(\"Test accuracy:\", eval_results[1])\n",
        "\n",
        "# Predict\n",
        "new_texts = [\n",
        "    'The serum feels very sticky on my skin.' ,\n",
        "    'It is really works in my skin.',\n",
        "    'The texture is great, but the brightening effect takes a long time to show.'\n",
        "]\n",
        "\n",
        "new_encodings = tokenizer(new_texts, truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
        "predictions = model.predict([new_encodings['input_ids'], new_encodings['attention_mask']])\n",
        "logits = predictions.logits\n",
        "predicted_labels = tf.argmax(logits, axis=1).numpy()\n",
        "predicted_sentiments = [list(label_mapping.keys())[list(label_mapping.values()).index(label)] for label in predicted_labels]\n",
        "print(\"Predicted sentiments:\", predicted_sentiments)\n",
        "\n",
        "# Save model weights\n",
        "model.save_weights(MODEL_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTb0l0tSqjtS",
        "outputId": "9e1f5d2d-f660-4b28-804d-8eceac73fba7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFAlbertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFAlbertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "11/11 [==============================] - 18s 208ms/step - loss: 2.3886 - accuracy: 0.3761\n",
            "Epoch 2/20\n",
            "11/11 [==============================] - 2s 215ms/step - loss: 1.0489 - accuracy: 0.5433\n",
            "Epoch 3/20\n",
            "11/11 [==============================] - 2s 204ms/step - loss: 0.7508 - accuracy: 0.7493\n",
            "Epoch 4/20\n",
            "11/11 [==============================] - 2s 204ms/step - loss: 0.6465 - accuracy: 0.7821\n",
            "Epoch 5/20\n",
            "11/11 [==============================] - 2s 204ms/step - loss: 0.4521 - accuracy: 0.8806\n",
            "Epoch 6/20\n",
            "11/11 [==============================] - 2s 203ms/step - loss: 0.9340 - accuracy: 0.7761\n",
            "Epoch 7/20\n",
            "11/11 [==============================] - 2s 209ms/step - loss: 0.5442 - accuracy: 0.9194\n",
            "Epoch 8/20\n",
            "11/11 [==============================] - 2s 210ms/step - loss: 0.3723 - accuracy: 0.9313\n",
            "Epoch 9/20\n",
            "11/11 [==============================] - 2s 205ms/step - loss: 0.3661 - accuracy: 0.9373\n",
            "Epoch 10/20\n",
            "11/11 [==============================] - 2s 205ms/step - loss: 1.0052 - accuracy: 0.5731\n",
            "Epoch 11/20\n",
            "11/11 [==============================] - 2s 204ms/step - loss: 0.5065 - accuracy: 0.6806\n",
            "Epoch 12/20\n",
            "11/11 [==============================] - 2s 206ms/step - loss: 0.4499 - accuracy: 0.7403\n",
            "Epoch 13/20\n",
            "11/11 [==============================] - 2s 213ms/step - loss: 0.4348 - accuracy: 0.8358\n",
            "Epoch 14/20\n",
            "11/11 [==============================] - 2s 205ms/step - loss: 0.4249 - accuracy: 0.8687\n",
            "Epoch 15/20\n",
            "11/11 [==============================] - 2s 205ms/step - loss: 0.4072 - accuracy: 0.8925\n",
            "Epoch 16/20\n",
            "11/11 [==============================] - 2s 205ms/step - loss: 0.3894 - accuracy: 0.9343\n",
            "Epoch 17/20\n",
            "11/11 [==============================] - 2s 213ms/step - loss: 0.3755 - accuracy: 0.9254\n",
            "Epoch 18/20\n",
            "11/11 [==============================] - 3s 268ms/step - loss: 0.3297 - accuracy: 0.9642\n",
            "Epoch 19/20\n",
            "11/11 [==============================] - 2s 207ms/step - loss: 0.2958 - accuracy: 0.9582\n",
            "Epoch 20/20\n",
            "11/11 [==============================] - 2s 207ms/step - loss: 0.2468 - accuracy: 0.9642\n",
            "3/3 [==============================] - 3s 89ms/step - loss: 0.7100 - accuracy: 0.9048\n",
            "Test loss: 0.7099828124046326\n",
            "Test accuracy: 0.9047619104385376\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "Predicted sentiments: ['Negative', 'Positive', 'Neutral']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Module providing a train pipelines for sentiment analysis\"\"\"\n",
        "\n",
        "from transformers import AlbertTokenizer, TFAlbertForSequenceClassification\n",
        "import tensorflow as tf\n",
        "\n",
        "# Download ALBERT Pre-trained Model\n",
        "label_mapping = {'Positive': 0, 'Neutral': 1, 'Negative': 2}\n",
        "# label_mapping = {'Very Positive': 0, 'Very Negative': 1, 'Mixed': 2, 'Positive': 3, 'Negative': 4, 'Neutral': 5}\n",
        "MAX_LENGTH = 1000\n",
        "NUM_LABELS = 3 # Adjust num_labels based on the number of sentiments\n",
        "MODEL_PATH = 'models/my-product-data-202412221621.h5'\n",
        "BASE_PRETRAINED_MODEL='albert-base-v2'\n",
        "\n",
        "\n",
        "tokenizer = AlbertTokenizer.from_pretrained(BASE_PRETRAINED_MODEL)\n",
        "model = TFAlbertForSequenceClassification.from_pretrained(BASE_PRETRAINED_MODEL, num_labels=NUM_LABELS)\n",
        "model.load_weights(MODEL_PATH)\n",
        "\n",
        "new_texts = [\n",
        "    'This brightening serum made my skin feel so oily and I see tehre is no brightening effect after weeks of use.',\n",
        "    'It is an average serum. My skin feels smooth, but the brightening effect is subtle.',\n",
        "    'I really love how this serum has brightened my complexion and reduced redness on my face.',\n",
        "    'Unfortunately, this serum caused irritation, and I had to discontinue using it.',\n",
        "    'It is okay. My skin feels hydrated, but I was expecting a more noticeable improvement.',\n",
        "    'After a month of using this brightening serum, I can see a visible glow in my skin!',\n",
        "    'This serum is too heavy for my skin type, and I did not notice any brightening effects.',\n",
        "    'It is fine, it brightens my skin a little, but I expected faster results.',\n",
        "    'This brightening serum has transformed my dull skin into a radiant glow!',\n",
        "    'I did not see any difference after using this serum for several weeks, which was disappointing.'\n",
        "]\n",
        "\n",
        "new_encodings = tokenizer(new_texts, truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
        "\n",
        "new_input_ids = new_encodings['input_ids'].numpy()\n",
        "new_attention_mask = new_encodings['attention_mask'].numpy()\n",
        "\n",
        "\n",
        "# Mengambil logits dari TFSequenceClassifierOutput dan lakukan predictions\n",
        "predictions = model.predict([new_input_ids, new_attention_mask]) # type: ignore\n",
        "logits = predictions.logits\n",
        "predicted_labels = tf.argmax(logits, axis=1).numpy()\n",
        "predicted_sentiments = [list(label_mapping.keys())[list(label_mapping.values()).index(label)] for label in predicted_labels]\n",
        "print(f'Predicted sentiments: {predicted_sentiments}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBxOUE0bvnwq",
        "outputId": "50da0a03-0aa0-455e-ab02-73f3a7fce183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFAlbertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFAlbertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 3s 3s/step\n",
            "Predicted sentiments: ['Positive', 'Neutral', 'Positive', 'Negative', 'Neutral', 'Positive', 'Negative', 'Neutral', 'Positive', 'Negative']\n"
          ]
        }
      ]
    }
  ]
}